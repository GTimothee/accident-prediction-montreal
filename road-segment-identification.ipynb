{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from road_network import fetch_road_network, extract_road_segments_DF\n",
    "from accidents_montreal import fetch_accidents_montreal, extract_accidents_montreal_dataframe\n",
    "from weather import get_weather\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import pow, col, min\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"Accident Prediction\")\n",
    "        .getOrCreate())\n",
    "    return spark\n",
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip fetching road network: already downloaded\n",
      "Skip extraction of road network dataframe: already done, reading from file\n"
     ]
    }
   ],
   "source": [
    "fetch_road_network()\n",
    "rnd = extract_road_segments_DF(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip fetching montreal accidents dataset: already downloaded\n",
      "Skip extraction of accidents montreal dataframe: already done, reading from file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['street_name',\n",
       " 'street_type',\n",
       " 'center_long',\n",
       " 'center_lat',\n",
       " 'coord_long',\n",
       " 'coord_lat']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_accidents_montreal()\n",
    "amd = extract_accidents_montreal_dataframe(spark)\n",
    "rnd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, sin, cos, radians, atan2, sqrt, rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_centers = (rnd\n",
    "                .select(['street_name', 'street_type', 'center_long', 'center_lat'])\n",
    "                .drop_duplicates()\n",
    "                .sample(0.1)\n",
    "                .persist())\n",
    "sample_accidents = amd.sample(0.05).withColumn('ID', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=26, street_name='Boulevard Métropolitain', street_type='Primary', distance_rank=1, distance=89.95913383683441),\n",
       " Row(ID=26, street_name='Boulevard Ray-Lawson', street_type='Primary', distance_rank=2, distance=139.34510529889405),\n",
       " Row(ID=26, street_name='Avenue Azilda', street_type='Secondary', distance_rank=3, distance=162.81337967265424),\n",
       " Row(ID=26, street_name='Boulevard Métropolitain', street_type='Primary', distance_rank=4, distance=191.1244951967038),\n",
       " Row(ID=26, street_name='Place Pigeon', street_type='Secondary', distance_rank=5, distance=198.4549155752083)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source: https://www.movable-type.co.uk/scripts/latlong.html\n",
    "earth_diameter = 6371 * 2 * 1000\n",
    "distance_inter = (pow(sin(radians(col('LOC_LAT') - col('center_lat'))/2), 2)\n",
    "            + (pow(sin(radians(col('LOC_LONG') - col('center_long'))/2), 2) \n",
    "                * cos(radians(col('LOC_LAT'))) * cos(radians(col('center_lat')))))\n",
    "distance_measure = atan2(sqrt(col('distance_inter')), sqrt(1-col('distance_inter')))\n",
    "\n",
    "accidentWindow = Window.partitionBy(\"ID\").orderBy(\"distance_measure\")\n",
    "cart = (sample_accidents\n",
    "        .select(['LOC_LAT', 'LOC_LONG', 'ID'])\n",
    "        .crossJoin(road_centers)\n",
    "        .withColumn('distance_inter', distance_inter)\n",
    "        .withColumn('distance_measure', distance_measure)\n",
    "        .select('ID', 'street_name', 'street_type', 'distance_measure',\n",
    "               rank().over(accidentWindow).alias('distance_rank'))\n",
    "        .filter(col('distance_rank') <= 5)\n",
    "        .withColumn('distance', col('distance_measure')*earth_diameter)\n",
    "        .drop('min_distance_measure', 'distance_measure'))\n",
    "\n",
    "cart.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of the distance measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         distance|\n",
      "+-----------------+\n",
      "|              0.0|\n",
      "|5918.185064088762|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window\n",
    "import pandas as pd\n",
    "\n",
    "# First example distance between identical points, and second example between London and Arlington \n",
    "DF = pd.DataFrame({'LOC_LAT': [0, 51.5],\n",
    "                   'LOC_LONG': [0, 0],\n",
    "                   'center_lat': [0, 38.8],\n",
    "                   'center_long': [0, -77.1]\n",
    "                  })\n",
    "df = (spark.createDataFrame(DF)\n",
    "        .withColumn('distance_inter', distance_inter)\n",
    "        .withColumn('distance_measure', distance_measure)\n",
    "        .withColumn('distance', col('distance_measure') * earth_diameter)\n",
    "         .select('distance'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of the top k selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  a|  b|  c|  y|\n",
      "+---+---+---+---+\n",
      "|  1|  3|  1|  1|\n",
      "|  1|  2|  2|  2|\n",
      "|  3|  1|  7|  1|\n",
      "|  3|  2|  8|  2|\n",
      "|  2|  1|  4|  1|\n",
      "|  2|  2|  5|  2|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window\n",
    "import pandas as pd\n",
    "\n",
    "k = 2\n",
    "DF = pd.DataFrame({'a': [1,1,1,2,2,2,3,3,3],\n",
    "                   'b': [1,2,3,1,2,3,1,2,3],\n",
    "                   'c': [3,2,1,4,5,6,7,8,9]\n",
    "                  })\n",
    "\n",
    "df = spark.createDataFrame(DF)\n",
    "\n",
    "window = Window.partitionBy(\"a\").orderBy(\"c\")\n",
    "\n",
    "df.select('a', 'b', 'c', f.rank().over(window).alias('y')).filter(col('y') <= k).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accident-prediction-montreal",
   "language": "python",
   "name": "accident-prediction-montreal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
