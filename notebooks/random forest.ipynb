{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.insert(0, '/home/hantoine/concordia/bigdata/project/spark/python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session created\n",
      "Parameters:\n",
      "\tspark.driver.port: 46721\n",
      "\tspark.network.timeout: 300s\n",
      "\tspark.app.id: local-1555620904903\n",
      "\tspark.master: local[10]\n",
      "\tspark.executor.id: driver\n",
      "\tspark.driver.host: laptop-hantoine.wireless.concordia.ca\n",
      "\tspark.app.name: Accident prediction\n",
      "\tspark.cleaner.periodicGC.interval: 5min\n",
      "\tspark.serializer: org.apache.spark.serializer.KryoSerializer\n",
      "\tspark.driver.memory: 7g\n",
      "\tspark.rdd.compress: True\n",
      "\tspark.serializer.objectStreamReset: 100\n",
      "\tspark.submit.pyFiles: \n",
      "\tspark.submit.deployMode: client\n",
      "\tspark.ui.showConsoleProgress: true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.0.0-SNAPSHOT'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocess import get_negative_samples, get_positive_samples\n",
    "from utils import init_spark\n",
    "spark = init_spark()\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_samples = get_negative_samples(spark).sample(0.2)\n",
    "pos_samples = get_positive_samples(spark)\n",
    "# pos_samples.write.parquet('data/pos_samples_0.2.parquet')\n",
    "# neg_samples.write.parquet('data/neg_samples_0.2.parquet')\n",
    "# pos_samples = spark.read.parquet('data/pos_samples_0.2.parquet')\n",
    "# neg_samples = spark.read.parquet('data/neg_samples_0.2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "# from datetime import datetime\n",
    "# from weather import get_weather_df\n",
    "# pos_samples.select('date', 'visibility').show()\n",
    "# weather = get_weather_df(spark, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_samples.select(avg('visibility'), stddev('visibility')).show()\n",
    "# pos_samples.select(avg('visibility'), stddev('visibility')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_samples.filter(col('date') == datetime.fromisoformat('2016-06-19')).select(avg('visibility'), stddev('visibility'), count('sample_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from road_network import get_road_features_df\n",
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql import Window\n",
    "# road = get_road_features_df(spark)\n",
    "# col_name = 'street_id'\n",
    "# street_risk_rank = \\\n",
    "#     (pos_samples\n",
    "#      .select(col_name)\n",
    "#      .na.fill(\"unknown\")\n",
    "#      .groupBy(col_name).count()\n",
    "#      .withColumn('risk', col('count') / lit(pos_samples.count()))\n",
    "#      .drop('count')\n",
    "#      .join(road.select('street_id'), 'street_id', 'outer')\n",
    "#      .na.fill(0, ['risk'])\n",
    "#      .withColumn(col_name + '_indexed',\n",
    "#                  row_number().over(Window.orderBy(col('risk').desc())))\n",
    "#      .drop('risk')\n",
    "#     )\n",
    "# # street_risk_rank.orderBy('street_id_indexed').show(n=100)\n",
    "\n",
    "# pos_samples = pos_samples.join(street_risk_rank, 'street_id')\n",
    "# neg_samples = neg_samples.join(street_risk_rank, 'street_id')\n",
    "# neg_samples.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "train_pos = \\\n",
    "    (pos_samples\n",
    "    .filter(col('date') < datetime.fromisoformat('2017-01-01')))\n",
    "train_neg = \\\n",
    "    (neg_samples\n",
    "    .filter(col('date') < datetime.fromisoformat('2017-01-01')))\n",
    "test_pos = \\\n",
    "    (pos_samples\n",
    "    .filter(col('date') >= datetime.fromisoformat('2017-01-01')))\n",
    "test_neg = \\\n",
    "    (neg_samples\n",
    "    .filter(col('date') >= datetime.fromisoformat('2017-01-01')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113450"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "835269"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_neg.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip extracting road features: already done\n"
     ]
    }
   ],
   "source": [
    "from road_network import get_road_features_df\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "road = get_road_features_df(spark)\n",
    "street_accident_count = \\\n",
    "    (train_pos\n",
    "     .select('street_id')\n",
    "     .groupBy('street_id').count()\n",
    "     .join(road.select('street_id'), 'street_id', 'outer')\n",
    "     .na.fill(0, ['count'])\n",
    "     .withColumnRenamed('count', 'accident_count')\n",
    "    )\n",
    "\n",
    "train_pos = train_pos.join(street_accident_count, 'street_id')\n",
    "train_neg = train_neg.join(street_accident_count, 'street_id')\n",
    "test_pos = test_pos.join(street_accident_count, 'street_id')\n",
    "test_neg = test_neg.join(street_accident_count, 'street_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113450"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip extracting road features: already done\n",
      "Skip extracting road features: already done\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from preprocess import get_dataset_df, features_col, remove_positive_samples_from_negative_samples\n",
    "\n",
    "lvl_i, typ_i, train_set = get_dataset_df(spark, train_pos, train_neg)\n",
    "\n",
    "test_neg = remove_positive_samples_from_negative_samples(test_neg,\n",
    "                                                            test_pos)\n",
    "test_pos = test_pos.withColumn('label', lit(1.0))\n",
    "test_neg = test_neg.withColumn('label', lit(0.0))\n",
    "\n",
    "test_pos = test_pos.select(*test_neg.columns)\n",
    "test_set = test_pos.union(test_neg)\n",
    "test_set = test_set.join(lvl_i, 'street_level').drop('street_level')\n",
    "test_set = test_set.join(typ_i, 'street_type').drop('street_type')\n",
    "\n",
    "assembler = VectorAssembler(outputCol=\"features\",\n",
    "                                inputCols=features_col,\n",
    "                                handleInvalid='keep'\n",
    "                                )\n",
    "test_set = (assembler.transform(test_set)\n",
    "      .select('sample_id',\n",
    "              'street_id',\n",
    "              'date', 'hour', 'features', 'label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalance ratio: 7.362441604230939\n"
     ]
    }
   ],
   "source": [
    "imbalance_ratio = (train_neg.count()/train_pos.count())\n",
    "print(f\"Imbalance ratio: {imbalance_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from preprocess import get_dataset_df\n",
    "# df = get_dataset_df(spark, pos_samples, neg_samples)\n",
    "# df = df.persist()\n",
    "# df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col\n",
    "# from datetime import datetime\n",
    "# df_sample = df.filter(col('date') > datetime.fromisoformat('2017-01-01'))\n",
    "# df_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random_undersampler import RandomUnderSampler\n",
    "\n",
    "# (train_set, test_set) = df.randomSplit([0.7, 0.3])\n",
    "# ru = RandomUnderSampler().setIndexCol('id')\n",
    "# model = ru.fit(train_set)\n",
    "# train_set = model.transform(train_set)\n",
    "# test_set = model.transform(test_set)\n",
    "# print(train_set.count(), test_set.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import udf, col\n",
    "# from pyspark.sql.types import ArrayType, DoubleType\n",
    "# from random_forest import features_col\n",
    "# dayofweek_features = [f'{features_col[-1]}_{i}' for i in range(1, 8)]\n",
    "# feature_names = features_col[:-1] + dayofweek_features\n",
    "\n",
    "# def extract_X(samples):\n",
    "#     def to_array(col):\n",
    "#         def to_array_(v):\n",
    "#             return v.toArray().tolist()\n",
    "#         return udf(to_array_, ArrayType(DoubleType()))(col)\n",
    "#     X = (samples\n",
    "#          .select(to_array('features').alias('fa'))\n",
    "#          .select([col(\"fa\")[i] for i in range(26)]))\n",
    "#     X = X.toDF(*feature_names)\n",
    "#     #return X\n",
    "#     return X.toPandas()#.values\n",
    "\n",
    "# trainX = extract_X(train_set)\n",
    "# # testX = extract_X(test_set)\n",
    "# trainy = train_set.select('label').toPandas().values.T\n",
    "# # trainx = test_set.select('label').toPandas().values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainX.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_recall_curve\n",
    "# from sklearn.metrics import auc\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from matplotlib import pyplot\n",
    "\n",
    "# model = RandomForestClassifier(n_estimators=100)\n",
    "# model.fit(trainX, trainy)\n",
    "# # predict probabilities\n",
    "# probs = model.predict_proba(testX)\n",
    "# # keep probabilities for the positive outcome only\n",
    "# probs = probs[:, 1]\n",
    "# # predict class values\n",
    "# yhat = model.predict(testX)\n",
    "# # calculate precision-recall curve\n",
    "# precision, recall, thresholds = precision_recall_curve(testy, probs)\n",
    "# # calculate precision-recall AUC\n",
    "# auc = auc(recall, precision)\n",
    "# print('auc=%.3f' % auc)\n",
    "# # plot no skill\n",
    "# pyplot.plot([0, 1], [0.5, 0.5], linestyle='--')\n",
    "# # plot the precision-recall curve for the model\n",
    "# pyplot.plot(recall, precision, marker='.')\n",
    "# # show the plot\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "from random_undersampler import RandomUnderSampler\n",
    "from class_weighter import ClassWeighter\n",
    "\n",
    "\n",
    "\n",
    "# (train_set, test_set) = df.randomSplit([0.8, 0.2])\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\",\n",
    "                            featuresCol=\"features\",\n",
    "                            cacheNodeIds=True,\n",
    "                            maxDepth=17,\n",
    "                            impurity='entropy',\n",
    "                            featureSubsetStrategy='sqrt',\n",
    "                            weightCol='weight',\n",
    "                            minInstancesPerNode=10,\n",
    "                            numTrees=100,\n",
    "                            subsamplingRate=1.0,\n",
    "                            maxMemoryInMB=768\n",
    "                           )\n",
    "ru = RandomUnderSampler().setIndexCol('id').setTargetImbalanceRatio(1.0)\n",
    "cw = ClassWeighter().setClassWeight([1/imbalance_ratio, 1.0])\n",
    "pipeline = Pipeline().setStages([\n",
    "#                                  ru,\n",
    "                                 cw,\n",
    "                                 rf])\n",
    "model = pipeline.fit(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validation does not make sense anymore with the accident count feature\n",
    "\n",
    "keeping for now to compare RF to BRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramGrid = (ParamGridBuilder()\n",
    "#      .addGrid(rf.numTrees, [100])\n",
    "#     .addGrid(rf.featureSubsetStrategy, ['sqrt'])\n",
    "#     .addGrid(rf.impurity, ['entropy'])\n",
    "#     .addGrid(rf.maxDepth, [17])\n",
    "#      .addGrid(rf.minInstancesPerNode, [10])\n",
    "#      .addGrid(rf.subsamplingRate, [1.0])\n",
    "# #     .addGrid(ru.targetImbalanceRatio, [1.0, 2.0, 3.0, 5.0, 7.0])\n",
    "#     .addGrid(ru.targetImbalanceRatio, [1.0])\n",
    "# #     .addGrid(cw.classWeight, [[1/imbalance_ratio, 1.0]])\n",
    "#     .build())\n",
    "# PR_evaluator = BinaryClassificationEvaluator(labelCol=\"label\",\n",
    "#                                              rawPredictionCol=\"rawPrediction\",\n",
    "#                                              metricName=\"areaUnderPR\")\n",
    "# tvs = CrossValidator(estimator=pipeline,\n",
    "#                            estimatorParamMaps=paramGrid,\n",
    "#                            evaluator=PR_evaluator,\n",
    "# #                            trainRatio=0.7,\n",
    "#                            collectSubModels=True)\n",
    "# model = tvs.fit(train_set)\n",
    "# model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelineModel' object has no attribute 'bestModel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c91826640098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{k.name}: {params[k]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelineModel' object has no attribute 'bestModel'"
     ]
    }
   ],
   "source": [
    "def print_params(model):\n",
    "    for stage in model.stages:\n",
    "        params = stage.extractParamMap()\n",
    "        for k in params:\n",
    "            print(f'{k.name}: {params[k]}')\n",
    "\n",
    "print_params(model.bestModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = model.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since test_set does not contain ids and random undersmapler is expecting one\n",
    "# and we don't want to add id to test_set since some of them could match the ones in the training set \n",
    "# which would result in test examples to be removed\n",
    "# And Since random sampler should be identiy for the test set, let juste call transform of the rf stage\n",
    "predictions = model.stages[1].transform(test_set).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate_binary_classifier\n",
    "evaluate_binary_classifier(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC_evaluator = \\\n",
    "    BinaryClassificationEvaluator(labelCol=\"label\",\n",
    "                                  rawPredictionCol=\"rawPrediction\",\n",
    "                                  metricName=\"areaUnderROC\")\n",
    "area_under_ROC = ROC_evaluator.evaluate(predictions)\n",
    "area_under_ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.transform(train_set).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate_binary_classifier\n",
    "evaluate_binary_classifier(train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC_evaluator = \\\n",
    "    BinaryClassificationEvaluator(labelCol=\"label\",\n",
    "                                  rawPredictionCol=\"rawPrediction\",\n",
    "                                  metricName=\"areaUnderROC\")\n",
    "area_under_ROC = ROC_evaluator.evaluate(train_predictions)\n",
    "area_under_ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_forest import features_col\n",
    "import pandas as pd\n",
    "fi = pd.DataFrame(model\n",
    "                  .bestModel\n",
    "                  .stages[1]\n",
    "                  .featureImportances\n",
    "                  .toArray())\n",
    "fi.index = features_col\n",
    "fi.columns = ['Feature Importances']\n",
    "fi = fi.sort_values(by=['Feature Importances'], ascending=False)\n",
    "fi.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_forest import compute_precision_recall_graph\n",
    "precision_recall = compute_precision_recall_graph(predictions, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random_forest import compute_precision_recall\n",
    "# compute_precision_recall(predictions, 0.83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision_recall.to_csv('data/precision_recall_brf_cheat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall.plot(x='Threshold', title='RF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision_recall.plot(x='Recall', y='Precision', ylim=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision_recall.to_csv('results/brf_pr_tab.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accident-prediction-montreal",
   "language": "python",
   "name": "accident-prediction-montreal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
