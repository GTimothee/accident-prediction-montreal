{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hantoine/Documents/Cours/Concordia/2019-Winter/SOEN_691_Big_Data_Analytics/project/accident-prediction-montreal\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.insert(0, '/home/hantoine/concordia/bigdata/project/spark/python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session created\n",
      "Parameters:\n",
      "\tspark.driver.extraClassPath: ./data/xgboost4j-spark-0.72.jar:./data/xgboost4j-0.72.jar\n",
      "\tspark.network.timeout: 300s\n",
      "\tspark.app.id: local-1555946583229\n",
      "\tspark.master: local[10]\n",
      "\tspark.executor.id: driver\n",
      "\tspark.driver.host: laptop-hantoine.wireless.concordia.ca\n",
      "\tspark.app.name: Accident prediction\n",
      "\tspark.cleaner.periodicGC.interval: 5min\n",
      "\tspark.serializer: org.apache.spark.serializer.KryoSerializer\n",
      "\tspark.driver.memory: 7g\n",
      "\tspark.driver.port: 35023\n",
      "\tspark.rdd.compress: True\n",
      "\tspark.serializer.objectStreamReset: 100\n",
      "\tspark.submit.pyFiles: \n",
      "\tspark.submit.deployMode: client\n",
      "\tspark.ui.showConsoleProgress: true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.0.0-SNAPSHOT'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocess import get_negative_samples, get_positive_samples\n",
    "from utils import init_spark\n",
    "spark = init_spark()\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from weather import get_weather_df\n",
    "# from pyspark.sql.functions import col\n",
    "# weather_df = get_weather_df(spark, None)\n",
    "# (weather_df\n",
    "#  .filter((col('date') == datetime.fromisoformat('2014-01-17').date()) & (col('hour') == 15))\n",
    "#  .select('station_id', 'rel_hum', 'visibility', 'stn_press', 'temp', 'risky_weather')\n",
    "#  .dropna(how='all', subset=['rel_hum', 'visibility', 'stn_press', 'temp'])\n",
    "#  .show())\n",
    "\n",
    "# from preprocess import get_weather_information\n",
    "# from weather import get_weather_df\n",
    "\n",
    "# weather_df = get_weather_df(spark, None)\n",
    "# (get_weather_information(sample, weather_df)\n",
    "#  .select('rel_hum', 'visibility', 'stn_press', 'temp', 'risky_weather')\n",
    "#  .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# sample = spark.createDataFrame([(0, datetime.fromisoformat('2014-01-17').date(), 15, -73.66106428971901, 45.48674117457887)],\n",
    "#                       ['sample_id', 'date', 'hour', 'loc_long', 'loc_lat'])\n",
    "# sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_samples = get_negative_samples(spark).sample(0.5)\n",
    "pos_samples = get_positive_samples(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import concat, lit, col\n",
    "# weather_sum_pos = (pos_samples\n",
    "#                    .select('visibility', 'stn_press', 'hmdx', 'rel_hum','dew_point_temp',\n",
    "#                            'risky_weather', 'temp', 'wind_spd', 'wind_dir')\n",
    "#                    .summary()\n",
    "#                    .withColumn('v', concat(col('summary'), lit(' of positives'))))\n",
    "# weather_sum_neg = (neg_samples\n",
    "#                    .select('visibility', 'stn_press', 'hmdx', 'rel_hum','dew_point_temp',\n",
    "#                            'risky_weather', 'temp', 'wind_spd', 'wind_dir')\n",
    "#                    .summary()\n",
    "#                    .withColumn('v', concat(col('summary'), lit(' of negatives'))))\n",
    "# weather_sum = weather_sum_pos.union(weather_sum_neg)#.orderBy('summary').drop('summary')\n",
    "# weather_sum.toPandas().set_index('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accidents_montreal import get_accident_df\n",
    "# from weather import get_weather_station_coords_df\n",
    "# from operator import and_\n",
    "# acc = get_accident_df(spark)\n",
    "# weather_stations = get_weather_station_coords_df(spark, None)\n",
    "# def write_to_KML_file(row):\n",
    "#     with open(f'./data/kml_stations/{str(row.station_id)}.kml', 'w') as file:\n",
    "#         file.write(f\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "# <kml xmlns=\"http://www.opengis.net/kml/2.2\">\n",
    "#   <Document>\n",
    "#   <Placemark>\n",
    "#     <name>Station {row.station_id}</name>\n",
    "#     <Point>\n",
    "#       <coordinates>{row.station_long},{row.station_lat},0</coordinates>\n",
    "#     </Point>\n",
    "#   </Placemark>\n",
    "#   </Document>\n",
    "# </kml>\n",
    "# \"\"\")\n",
    "#     return True\n",
    "# weather_stations.rdd.map(write_to_KML_file).reduce(and_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalance ratio: 8.62468305958108\n"
     ]
    }
   ],
   "source": [
    "imbalance_ratio = (neg_samples.count()/pos_samples.count())\n",
    "print(f\"Imbalance ratio: {imbalance_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip extracting road features: already done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000192"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocess import get_dataset_df\n",
    "train_set, test_set = get_dataset_df(spark, pos_samples, neg_samples)\n",
    "train_set, test_set = train_set.persist(), test_set.persist()\n",
    "train_set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col\n",
    "# from datetime import datetime\n",
    "# df_sample = df.filter(col('date') > datetime.fromisoformat('2017-01-01'))\n",
    "# df_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random_undersampler import RandomUnderSampler\n",
    "\n",
    "# (train_set, test_set) = df.randomSplit([0.7, 0.3])\n",
    "# ru = RandomUnderSampler().setIndexCol('id')\n",
    "# model = ru.fit(train_set)\n",
    "# train_set = model.transform(train_set)\n",
    "# test_set = model.transform(test_set)\n",
    "# print(train_set.count(), test_set.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import udf, col\n",
    "# from pyspark.sql.types import ArrayType, DoubleType\n",
    "# from random_forest import features_col\n",
    "# dayofweek_features = [f'{features_col[-1]}_{i}' for i in range(1, 8)]\n",
    "# feature_names = features_col[:-1] + dayofweek_features\n",
    "\n",
    "# def extract_X(samples):\n",
    "#     def to_array(col):\n",
    "#         def to_array_(v):\n",
    "#             return v.toArray().tolist()\n",
    "#         return udf(to_array_, ArrayType(DoubleType()))(col)\n",
    "#     X = (samples\n",
    "#          .select(to_array('features').alias('fa'))\n",
    "#          .select([col(\"fa\")[i] for i in range(26)]))\n",
    "#     X = X.toDF(*feature_names)\n",
    "#     #return X\n",
    "#     return X.toPandas()#.values\n",
    "\n",
    "# trainX = extract_X(train_set)\n",
    "# # testX = extract_X(test_set)\n",
    "# trainy = train_set.select('label').toPandas().values.T\n",
    "# # trainx = test_set.select('label').toPandas().values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainX.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_recall_curve\n",
    "# from sklearn.metrics import auc\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from matplotlib import pyplot\n",
    "\n",
    "# model = RandomForestClassifier(n_estimators=100)\n",
    "# model.fit(trainX, trainy)\n",
    "# # predict probabilities\n",
    "# probs = model.predict_proba(testX)\n",
    "# # keep probabilities for the positive outcome only\n",
    "# probs = probs[:, 1]\n",
    "# # predict class values\n",
    "# yhat = model.predict(testX)\n",
    "# # calculate precision-recall curve\n",
    "# precision, recall, thresholds = precision_recall_curve(testy, probs)\n",
    "# # calculate precision-recall AUC\n",
    "# auc = auc(recall, precision)\n",
    "# print('auc=%.3f' % auc)\n",
    "# # plot no skill\n",
    "# pyplot.plot([0, 1], [0.5, 0.5], linestyle='--')\n",
    "# # plot the precision-recall curve for the model\n",
    "# pyplot.plot(recall, precision, marker='.')\n",
    "# # show the plot\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "from random_undersampler import RandomUnderSampler\n",
    "from class_weighter import ClassWeighter\n",
    "\n",
    "\n",
    "\n",
    "# (train_set, test_set) = df.randomSplit([0.8, 0.2])\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\",\n",
    "                            featuresCol=\"features\",\n",
    "                            cacheNodeIds=True,\n",
    "                            maxDepth=17,\n",
    "                            impurity='entropy',\n",
    "                            featureSubsetStrategy='sqrt',\n",
    "                             weightCol='weight',\n",
    "                            minInstancesPerNode=10,\n",
    "                            numTrees=100,\n",
    "                            subsamplingRate=1.0,\n",
    "                            maxMemoryInMB=768\n",
    "                           )\n",
    "ru = RandomUnderSampler().setIndexCol('id').setTargetImbalanceRatio(1.0)\n",
    "cw = ClassWeighter().setClassWeight([1/imbalance_ratio, 1.0])\n",
    "pipeline = Pipeline().setStages([\n",
    "#                                  ru,\n",
    "                                 cw,\n",
    "                                 rf])\n",
    "model = pipeline.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramGrid = (ParamGridBuilder()\n",
    "#      .addGrid(rf.numTrees, [100])\n",
    "#     .addGrid(rf.featureSubsetStrategy, ['sqrt'])\n",
    "#     .addGrid(rf.impurity, ['entropy'])\n",
    "#     .addGrid(rf.maxDepth, [17])\n",
    "#      .addGrid(rf.minInstancesPerNode, [10])\n",
    "#      .addGrid(rf.subsamplingRate, [1.0])\n",
    "# #     .addGrid(ru.targetImbalanceRatio, [1.0, 2.0, 3.0, 5.0, 7.0])\n",
    "#     .addGrid(ru.targetImbalanceRatio, [1.0])\n",
    "# #     .addGrid(cw.classWeight, [[1/imbalance_ratio, 1.0]])\n",
    "#     .build())\n",
    "# PR_evaluator = BinaryClassificationEvaluator(labelCol=\"label\",\n",
    "#                                              rawPredictionCol=\"rawPrediction\",\n",
    "#                                              metricName=\"areaUnderPR\")\n",
    "# tvs = CrossValidator(estimator=pipeline,\n",
    "#                            estimatorParamMaps=paramGrid,\n",
    "#                            evaluator=PR_evaluator,\n",
    "# #                            trainRatio=0.7,\n",
    "#                            collectSubModels=True)\n",
    "# model = tvs.fit(train_set)\n",
    "# model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_params(model):\n",
    "    for stage in model.stages:\n",
    "        params = stage.extractParamMap()\n",
    "        for k in params:\n",
    "            print(f'{k.name}: {params[k]}')\n",
    "\n",
    "print_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = model.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since test_set does not contain ids and random undersmapler is expecting one\n",
    "# and we don't want to add id to test_set since some of them could match the ones in the training set \n",
    "# which would result in test examples to be removed\n",
    "# And Since random sampler should be identiy for the test set, let juste call transform of the rf stage\n",
    "predictions = model.stages[1].transform(test_set).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate_binary_classifier\n",
    "evaluate_binary_classifier(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC_evaluator = \\\n",
    "    BinaryClassificationEvaluator(labelCol=\"label\",\n",
    "                                  rawPredictionCol=\"rawPrediction\",\n",
    "                                  metricName=\"areaUnderROC\")\n",
    "area_under_ROC = ROC_evaluator.evaluate(predictions)\n",
    "area_under_ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.transform(train_set).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate_binary_classifier\n",
    "evaluate_binary_classifier(train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC_evaluator = \\\n",
    "    BinaryClassificationEvaluator(labelCol=\"label\",\n",
    "                                  rawPredictionCol=\"rawPrediction\",\n",
    "                                  metricName=\"areaUnderROC\")\n",
    "area_under_ROC = ROC_evaluator.evaluate(train_predictions)\n",
    "area_under_ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_forest import features_col\n",
    "import pandas as pd\n",
    "fi = pd.DataFrame(model\n",
    "                  .stages[1]\n",
    "                  .featureImportances\n",
    "                  .toArray())\n",
    "fi.index = features_col\n",
    "fi.columns = ['Feature Importances']\n",
    "fi = fi.sort_values(by=['Feature Importances'], ascending=False)\n",
    "fi.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_forest import compute_precision_recall_graph\n",
    "precision_recall = compute_precision_recall_graph(predictions, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall.plot(x='Threshold', title='RF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision_recall.plot(x='Recall', y='Precision', ylim=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision_recall.to_csv('results/rf_pr_tab.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accident-prediction-montreal",
   "language": "python",
   "name": "accident-prediction-montreal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
