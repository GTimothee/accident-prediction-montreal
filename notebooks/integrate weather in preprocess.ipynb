{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accident-montreal-documentation.pdf  road-network\r\n",
      "accidents-montreal.lock\t\t     road-network.lock\r\n",
      "accidents-montreal.parquet\t     road-network.parquet\r\n",
      "accidents-montreal.zip\t\t     weather\r\n"
     ]
    }
   ],
   "source": [
    "! ls data\n",
    "! rm -rf data/weather_backup.parquet\n",
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from accidents_montreal import fetch_accidents_montreal,\\\n",
    "                               extract_accidents_montreal_df\n",
    "from road_network import fetch_road_network, extract_road_segments_df\n",
    "from weather import get_weather\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import atan2, sqrt, row_number, cos, sin, radians,\\\n",
    "                                  col, rank, avg\n",
    "from pyspark.sql import functions, types, Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip fetching montreal accidents dataset: already downloaded\n",
      "Skip extraction of accidents montreal dataframe: already done, reading from file\n"
     ]
    }
   ],
   "source": [
    "def init_spark():\n",
    "    return (SparkSession\n",
    "            .builder\n",
    "            .getOrCreate())\n",
    "\n",
    "\n",
    "# init spark\n",
    "spark = init_spark()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# retrieve datasets\n",
    "fetch_accidents_montreal()\n",
    "accidents_df = extract_accidents_montreal_df(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "non_num = ['TempFlag',\n",
    "                    'DewPointTempFlag',\n",
    "                    'RelHumFlag',\n",
    "                    'WindDirFlag',\n",
    "                    'WindSpdFlag',\n",
    "                    'VisibilityFlag',\n",
    "                    'StnPressFlag',\n",
    "                    'HmdxFlag',\n",
    "                    'WindChillFlag']\n",
    "\n",
    "num = ['DewPointTemp°C',\n",
    "                'RelHum%',\n",
    "                'WindDir10sdeg',\n",
    "                'WindSpdkm/h',\n",
    "                'Visibilitykm',\n",
    "                'StnPresskPa',\n",
    "                'Hmdx',\n",
    "                'WindChill'] + ['Temp°C']\n",
    "\n",
    "schema = StructType(\n",
    "    [StructField(c, StringType(), True) for c in non_num]\n",
    "    + [StructField(c, FloatType(), True) for c in num]\n",
    "    + [StructField('Weather', ArrayType(elementType=StringType()), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def extract_date_val(i):\n",
    "    return udf(lambda val : val.split('/')[i])\n",
    "\n",
    "@udf\n",
    "def extract_hour(val):\n",
    "    return val.split('-')[0].split(':')[0]\n",
    "\n",
    "test = accidents_df.select('DT_ACCDN','LOC_LAT', 'LOC_LONG', 'HEURE_ACCDN')  \\\n",
    "                            .withColumn(\"year\", extract_date_val(0)(accidents_df.DT_ACCDN)) \\\n",
    "                            .withColumn(\"month\", extract_date_val(1)(accidents_df.DT_ACCDN)) \\\n",
    "                            .withColumn(\"day\", extract_date_val(2)(accidents_df.DT_ACCDN)) \\\n",
    "                            .withColumn(\"HEURE_ACCDN\", extract_hour(accidents_df.HEURE_ACCDN)) \\\n",
    "                            .drop('DT_ACCDN') \\\n",
    "                            .replace('Non précisé', '00')\n",
    "\n",
    "nb_elements_treated=0.0\n",
    "\n",
    "def ai(row):\n",
    "    new_row = get_weather(row.LOC_LAT, \n",
    "                                 row.LOC_LONG, \n",
    "                                 row.year, \n",
    "                                 row.month, \n",
    "                                 row.day, \n",
    "                                 row.HEURE_ACCDN)\n",
    "    \n",
    "    global nb_elements_treated\n",
    "    nb_elements_treated +=1\n",
    "    print('progress: ', nb_elements_treated*100/149886, '%')\n",
    "    return new_row\n",
    "    \n",
    "backup_file = 'data/weather_backup.parquet'    \n",
    "    \n",
    "import time\n",
    "t = time.time()\n",
    "test2 = spark.createDataFrame(test \\\n",
    "        .sample(0.5) \\\n",
    "        .rdd \\\n",
    "        .map(lambda row: ai(row)), schema).write.parquet(backup_file)\n",
    "t= time.time()-t\n",
    "print('total processing time: ', t)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#test for 1 row\n",
    "row1 = spark.createDataFrame(test.limit(150).collect(), test.columns) \\\n",
    "    .rdd \\\n",
    "    .first()\n",
    "rdd2 = sc.parallelize([row1]).map(lambda row: get_weather(int(row.LOC_LAT), \n",
    "                                 int(row.LOC_LONG), \n",
    "                                 int(row.year), \n",
    "                                 int(row.month), \n",
    "                                 int(row.day), \n",
    "                                 int(row.HEURE_ACCDN)))\n",
    "print(rdd2.first())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"test = spark.read.parquet(backup_file)\"\"\"\n",
    "\n",
    "#test3 = spark.createDataFrame(test2.collect(), list(a.asDict().keys()))\n",
    "\"\"\"test4.write.parquet('data/test.parquet')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"test.limit(1).collect()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\"df=extract_accidents_montreal_dataframe(spark)\n",
    "test=df.take(5)\n",
    "test = list(map(lambda e: (e.LOC_LAT, e.LOC_LONG), test))\n",
    "t= test[0]\n",
    "lat, long = t\n",
    "year, month, day = (2006,5,2)\n",
    "hour = 0\n",
    "print(get_weather(lat, long, year, month, day, hour))\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import os\n",
    "os.path.isfile('data/test.parquet')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'(' not in \"dsgarth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AccidentsPredictionEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
