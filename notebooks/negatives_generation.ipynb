{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/Documents/workspace/projects/accident-prediction-montreal\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from accidents_montreal import get_accident_df\n",
    "from utils import init_spark\n",
    "from road_network import get_road_df\n",
    "from weather import add_weather_columns, extract_year_month_day\n",
    "from pyspark.sql.functions import row_number, col, rank, avg, split, to_date, min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip fetching montreal accidents dataset: already downloaded\n",
      "Skip extraction of accidents montreal dataframe: already done, reading from file\n"
     ]
    }
   ],
   "source": [
    "acc_df = get_accident_df(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#acc_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012 2017\n"
     ]
    }
   ],
   "source": [
    "#create date_df\n",
    "dates_df = extract_year_month_day(acc_df.select('DT_ACCDN')\n",
    "                                  .withColumn('date', to_date(col('DT_ACCDN'), format='yyyy/MM/dd'))).persist()\n",
    "\n",
    "#get dates lower and upper bound\n",
    "rows = dates_df.select('year').summary(\"min\", \"max\").collect()\n",
    "min_ = rows[0]['year']\n",
    "max_ = rows[1]['year']\n",
    "print(min_, max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# function get_random_date\\nimport time\\nimport datetime\\nimport random\\nstart = \"01/01/\" + str(min_)\\nend = \"01/01/\" + str(max_)\\nprint(start, end)\\nstart_stamp = datetime.datetime.strptime(start, \"%d/%m/%Y\").timestamp()\\nend_stamp = datetime.datetime.strptime(end, \"%d/%m/%Y\").timestamp()\\nrandom.seed()\\nrandom_stamp = datetime.datetime.fromtimestamp(random.randint(start_stamp, end_stamp))\\ndate = random_stamp.strftime(\"%d/%m/%Y\")\\nhour = random.randint(0,12)\\nprint(date, hour)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# function get_random_date\n",
    "\n",
    "start = \"01/01/\" + str(min_)\n",
    "end = \"01/01/\" + str(max_)\n",
    "print(start, end)\n",
    "start_stamp = datetime.datetime.strptime(start, \"%d/%m/%Y\").timestamp()\n",
    "end_stamp = datetime.datetime.strptime(end, \"%d/%m/%Y\").timestamp()\n",
    "random.seed()\n",
    "random_stamp = datetime.datetime.fromtimestamp(random.randint(start_stamp, end_stamp))\n",
    "date = random_stamp.strftime(\"%d/%m/%Y\")\n",
    "hour = random.randint(0,12)\n",
    "print(date, hour)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate all dates from start to end\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "start = \"01/01/\" + str(min_)\n",
    "end = \"31/12/\" + str(max_)\n",
    "date = datetime.datetime.strptime(start, \"%d/%m/%Y\")\n",
    "\n",
    "dates = list()\n",
    "while(date != datetime.datetime.strptime(end, \"%d/%m/%Y\")): \n",
    "    date += datetime.timedelta(days=1)\n",
    "    for i in range(24):\n",
    "        dates.append((date.strftime(\"%Y-%m-%d\"), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take dataframe with 50 dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_dates:  52584\n",
      "nb_row in df:  52584\n",
      "+----------+----+\n",
      "|      date|hour|\n",
      "+----------+----+\n",
      "|2017-01-01|   0|\n",
      "|2017-01-01|   1|\n",
      "|2017-01-01|   2|\n",
      "|2017-01-01|   3|\n",
      "|2017-01-01|   4|\n",
      "|2017-01-01|   5|\n",
      "|2017-01-01|   6|\n",
      "|2017-01-01|   7|\n",
      "|2017-01-01|   8|\n",
      "|2017-01-01|   9|\n",
      "|2017-01-01|  10|\n",
      "|2017-01-01|  11|\n",
      "|2017-01-01|  12|\n",
      "|2017-01-01|  13|\n",
      "|2017-01-01|  14|\n",
      "|2017-01-01|  15|\n",
      "|2017-01-01|  16|\n",
      "|2017-01-01|  17|\n",
      "|2017-01-01|  18|\n",
      "|2017-01-01|  19|\n",
      "+----------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "[('date', 'string'), ('hour', 'bigint')]\n",
      "nb selected:  8760\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col, year\n",
    "print(\"nb_dates: \",len(dates))\n",
    "#spark.createDataFrame(dates, ['date', 'hour']).limit(10).show()\n",
    "dates_df = spark.createDataFrame(dates, ['date', 'hour'])\n",
    "print(\"nb_row in df: \", dates_df.count())\n",
    "dates_df = dates_df.withColumn('year', year(col('date'))).filter(col('year') == 2017).drop('year')\n",
    "#print(\"nb of rows selected: \", dates_df.count())\n",
    "dates_df.show()\n",
    "print(dates_df.dtypes)\n",
    "print(\"nb selected: \", dates_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# location\n",
    "location (route) => coordinates of a road \n",
    "\n",
    "loc_lat, loc_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip fetching road network: already downloaded\n",
      "Skip extraction of road network dataframe: already done, reading from file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'date_rdd = sc.parallelize(dates_df.rdd.take(20)).persist()\\nroad_rdd = sc.parallelize(road_df.rdd.take(20)).persist()\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand, monotonically_increasing_id\n",
    "from preprocess import generate_dates_df\n",
    "\n",
    "#dates_df = generate_dates_df(\"01/01/2012\", \"01/01/2017\", spark)\n",
    "road_df = (get_road_df(spark)\n",
    "               .select(['center_long', 'center_lat', 'street_id'])\n",
    "               .withColumnRenamed('center_lat', 'loc_lat')\n",
    "               .withColumnRenamed('center_long', 'loc_long')\n",
    "               .orderBy(rand()))\n",
    "#sc = spark.sparkContext\n",
    "\"\"\"date_rdd = sc.parallelize(dates_df.rdd.take(20)).persist()\n",
    "road_rdd = sc.parallelize(road_df.rdd.take(20)).persist()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get_road_df(spark).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb of roads 539293\n",
      "nb of roads selected 10\n"
     ]
    }
   ],
   "source": [
    "#road_rdd\n",
    "print(\"nb of roads\", road_df.count())\n",
    "road_df = road_df.limit(10)\n",
    "print(\"nb of roads selected\", road_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generation of negatives\n",
    "\n",
    "nb_roads(10) * nb_times (50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb negatives created :  500\n"
     ]
    }
   ],
   "source": [
    "negatives = dates_df.crossJoin(road_df)\n",
    "print('nb negatives created : ', negatives.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----------+----------+----------+\n",
      "|      date|hour|   loc_long|   loc_lat| street_id|\n",
      "+----------+----+-----------+----------+----------+\n",
      "|2012-01-02|   0|-73.5526897|45.4736431| 625435875|\n",
      "|2012-01-02|   0|-73.7609272|45.6791578| 355889550|\n",
      "|2012-01-02|   0|-73.4866435|45.7275524| 179742563|\n",
      "|2012-01-02|   0|-73.5164402| 45.705881|1765458084|\n",
      "|2012-01-02|   0|-73.6797578|45.5570462|1588959189|\n",
      "|2012-01-02|   0|-73.8508893|45.5077563|1411920530|\n",
      "|2012-01-02|   0|-73.4922642|45.4196451|  56163489|\n",
      "|2012-01-02|   0|-73.5058444|45.5847476|1149800350|\n",
      "|2012-01-02|   1|-73.6797578|45.5570462|1588959189|\n",
      "|2012-01-02|   1|-73.8508893|45.5077563|1411920530|\n",
      "|2012-01-02|   1|-73.4922642|45.4196451|  56163489|\n",
      "|2012-01-02|   1|-73.4771585|45.4493216|1678012933|\n",
      "|2012-01-02|   2|-73.6797578|45.5570462|1588959189|\n",
      "|2012-01-02|   2|-73.4922642|45.4196451|  56163489|\n",
      "|2012-01-02|   3|-73.5526897|45.4736431| 625435875|\n",
      "|2012-01-02|   3|-73.7609272|45.6791578| 355889550|\n",
      "|2012-01-02|   3|-73.5164402| 45.705881|1765458084|\n",
      "|2012-01-02|   3|-73.5058444|45.5847476|1149800350|\n",
      "|2012-01-02|   4|-73.7609272|45.6791578| 355889550|\n",
      "|2012-01-02|   4|-73.7610305|45.4481398| 764139817|\n",
      "+----------+----+-----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "negatives.sample(0.5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----------+----------+----------+-----------+\n",
      "|      date|hour|   loc_long|   loc_lat| street_id|accident_id|\n",
      "+----------+----+-----------+----------+----------+-----------+\n",
      "|2012-01-02|   0|-73.5526897|45.4736431| 625435875|          0|\n",
      "|2012-01-02|   0|-73.7609272|45.6791578| 355889550|          1|\n",
      "|2012-01-02|   0|-73.4866435|45.7275524| 179742563|          2|\n",
      "|2012-01-02|   0|-73.7610305|45.4481398| 764139817|          3|\n",
      "|2012-01-02|   0|-73.5164402| 45.705881|1765458084|          4|\n",
      "|2012-01-02|   0|-73.6797578|45.5570462|1588959189|          5|\n",
      "|2012-01-02|   0|-73.8508893|45.5077563|1411920530|          6|\n",
      "|2012-01-02|   0|-73.4922642|45.4196451|  56163489|          7|\n",
      "|2012-01-02|   0|-73.5058444|45.5847476|1149800350|          8|\n",
      "|2012-01-02|   0|-73.4771585|45.4493216|1678012933|          9|\n",
      "|2012-01-02|   1|-73.5526897|45.4736431| 625435875|         10|\n",
      "|2012-01-02|   1|-73.7609272|45.6791578| 355889550|         11|\n",
      "|2012-01-02|   1|-73.4866435|45.7275524| 179742563|         12|\n",
      "|2012-01-02|   1|-73.7610305|45.4481398| 764139817|         13|\n",
      "|2012-01-02|   1|-73.5164402| 45.705881|1765458084|         14|\n",
      "|2012-01-02|   1|-73.6797578|45.5570462|1588959189|         15|\n",
      "|2012-01-02|   1|-73.8508893|45.5077563|1411920530|         16|\n",
      "|2012-01-02|   1|-73.4922642|45.4196451|  56163489|         17|\n",
      "|2012-01-02|   1|-73.5058444|45.5847476|1149800350|         18|\n",
      "|2012-01-02|   1|-73.4771585|45.4493216|1678012933|         19|\n",
      "+----------+----+-----------+----------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#negatives_df = negatives.map(lambda row: row[0] + row[1]).toDF(['date','hour','loc_long','loc_lat', 'street_id']).persist()\n",
    "negatives = negatives.withColumn('accident_id', monotonically_increasing_id())\n",
    "negatives.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----------+----------+----------+-----------+----+\n",
      "|      date|hour|   loc_long|   loc_lat| street_id|accident_id|year|\n",
      "+----------+----+-----------+----------+----------+-----------+----+\n",
      "|2012-01-02|   0|-73.6056889|45.7435093|1869395346|          0|2012|\n",
      "|2012-01-02|   1|-73.6056889|45.7435093|1869395346|          1|2012|\n",
      "|2012-01-02|   2|-73.6056889|45.7435093|1869395346|          2|2012|\n",
      "|2012-01-02|   3|-73.6056889|45.7435093|1869395346|          3|2012|\n",
      "|2012-01-02|   4|-73.6056889|45.7435093|1869395346|          4|2012|\n",
      "|2012-01-02|   5|-73.6056889|45.7435093|1869395346|          5|2012|\n",
      "|2012-01-02|   6|-73.6056889|45.7435093|1869395346|          6|2012|\n",
      "|2012-01-02|   7|-73.6056889|45.7435093|1869395346|          7|2012|\n",
      "|2012-01-02|   8|-73.6056889|45.7435093|1869395346|          8|2012|\n",
      "|2012-01-02|   9|-73.6056889|45.7435093|1869395346|          9|2012|\n",
      "|2012-01-02|  10|-73.6056889|45.7435093|1869395346|         10|2012|\n",
      "|2012-01-02|  11|-73.6056889|45.7435093|1869395346|         11|2012|\n",
      "|2012-01-02|  12|-73.6056889|45.7435093|1869395346|         12|2012|\n",
      "|2012-01-02|  13|-73.6056889|45.7435093|1869395346|         13|2012|\n",
      "|2012-01-02|  14|-73.6056889|45.7435093|1869395346|         14|2012|\n",
      "|2012-01-02|  15|-73.6056889|45.7435093|1869395346|         15|2012|\n",
      "|2012-01-02|  16|-73.6056889|45.7435093|1869395346|         16|2012|\n",
      "|2012-01-02|  17|-73.6056889|45.7435093|1869395346|         17|2012|\n",
      "|2012-01-02|  18|-73.6056889|45.7435093|1869395346|         18|2012|\n",
      "|2012-01-02|  19|-73.6056889|45.7435093|1869395346|         19|2012|\n",
      "+----------+----+-----------+----------+----------+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col, year, month, dayofmonth\n",
    "from weather import extract_year_month_day\n",
    "negatives.withColumn('year', year(col('date'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plus_weather = add_weather_columns(spark, negatives)\n",
    "plus_weather.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from road_network import get_road_features_df\n",
    "road_features_df = get_road_features_df(spark, road_df=get_road_df(spark))\n",
    "plus_weather.drop('loc_long').drop('loc_lat').drop('Wind_Chill').join(road_features_df, 'street_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weather\n",
    "weather => year/month/day date + hour\n",
    "just query the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AccidentsPredictionEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
