@INPROCEEDINGS{Sharma2018, 
author={M. {Sharma} and T. {Glatard} and É. {Gélinas} and M. {Tagmouti} and B. {Jaumard}}, 
booktitle={2018 IEEE International Conference on Big Data (Big Data)}, 
title={Service failure prediction in supply-chain networks}, 
year={2018}, 
volume={}, 
number={}, 
pages={1827-1836}, 
keywords={customer satisfaction;customer services;data analysis;data mining;failure analysis;random forests;retailing;supply chain management;service failure prediction;supply-chain networks;delivery services;time window size;customer service;association rules;geographical location;human dispatchers;retailer company;random forests;data analysis;Feature extraction;Vegetation;Microsoft Windows;Companies;Vehicles;Big Data;Sensitivity}, 
doi={10.1109/BigData.2018.8622044}, 
ISSN={}, 
month={Dec},}
@Article{Breiman2001,
author="Breiman, Leo",
title="Random Forests",
journal="Machine Learning",
year="2001",
month="Oct",
day="01",
volume="45",
number="1",
pages="5--32",
abstract="Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.",
issn="1573-0565",
doi="10.1023/A:1010933404324",
url="https://doi.org/10.1023/A:1010933404324"
}
@article{Genuer2010,
title = "Variable selection using random forests",
journal = "Pattern Recognition Letters",
volume = "31",
number = "14",
pages = "2225 - 2236",
year = "2010",
issn = "0167-8655",
doi = "https://doi.org/10.1016/j.patrec.2010.03.014",
url = "http://www.sciencedirect.com/science/article/pii/S0167865510000954",
author = "Robin Genuer and Jean-Michel Poggi and Christine Tuleau-Malot",
keywords = "Random forests, Regression, Classification, Variable importance, Variable selection, High dimensional data",
abstract = "This paper proposes, focusing on random forests, the increasingly used statistical method for classification and regression problems introduced by Leo Breiman in 2001, to investigate two classical issues of variable selection. The first one is to find important variables for interpretation and the second one is more restrictive and try to design a good parsimonious prediction model. The main contribution is twofold: to provide some experimental insights about the behavior of the variable importance index based on random forests and to propose a strategy involving a ranking of explanatory variables using the random forests score of importance and a stepwise ascending variable introduction strategy."
}
@Article{Cerda2018,
author="Cerda, Patricio
and Varoquaux, Ga{\"e}l
and K{\'e}gl, Bal{\'a}zs",
title="Similarity encoding for learning with dirty categorical variables",
journal="Machine Learning",
year="2018",
month="Sep",
day="01",
volume="107",
number="8",
pages="1477--1494",
abstract="For statistical learning, categorical variables in a table are usually considered as discrete entities and encoded separately to feature vectors, e.g., with one-hot encoding. ``Dirty'' non-curated data give rise to categorical variables with a very high cardinality but redundancy: several categories reflect the same entity. In databases, this issue is typically solved with a deduplication step. We show that a simple approach that exposes the redundancy to the learning algorithm brings significant gains. We study a generalization of one-hot encoding, similarity encoding, that builds feature vectors from similarities across categories. We perform a thorough empirical validation on non-curated tables, a problem seldom studied in machine learning. Results on seven real-world datasets show that similarity encoding brings significant gains in predictive performance in comparison with known encoding methods for categories or strings, notably one-hot encoding and bag of character n-grams. We draw practical recommendations for encoding dirty categories: 3-gram similarity appears to be a good choice to capture morphological resemblance. For very high-cardinalities, dimensionality reduction significantly reduces the computational cost with little loss in performance: random projections or choosing a subset of prototype categories still outperform classic encoding approaches.",
issn="1573-0565",
doi="10.1007/s10994-018-5724-2",
url="https://doi.org/10.1007/s10994-018-5724-2"
}
@article{Chen2004,
author = {Chen, Chao and Breiman, Leo},
year = {2004},
month = {01},
pages = {},
title = {Using Random Forest to Learn Imbalanced Data},
journal = {University of California, Berkeley}
}
@article{Chang2005,
title = "Data mining of tree-based models to analyze freeway accident frequency",
journal = "Journal of Safety Research",
volume = "36",
number = "4",
pages = "365 - 375",
year = "2005",
issn = "0022-4375",
doi = "https://doi.org/10.1016/j.jsr.2005.06.013",
url = "http://www.sciencedirect.com/science/article/pii/S0022437505000708",
author = "Li-Yen Chang and Wen-Chieh Chen",
keywords = "Accident frequency, Freeway, Data mining, Classification and regression trees (CART), Negative binomial regression",
}
@inproceedings{QChen2016,
  title={Learning Deep Representation from Big and Heterogeneous Data for Traffic Accident Inference},
  author={Quanjun Chen and Xuan Song and Harutoshi Yamada and Ryosuke Shibasaki},
  booktitle={AAAI},
  year={2016}
}
@inproceedings{Valizadeh2009,
author = {Sima Valizadeh  and Behzad Moshiri  and Karim Salahshoor },
title = {Leak Detection in Transportation Pipelines Using Feature Extraction and KNN Classification},
booktitle = {Pipelines},
doi = {10.1061/41069(360)53},
year={2009}
}
@article{Christ2018,
title = "Time Series FeatuRe Extraction on basis of Scalable Hypothesis tests (tsfresh – A Python package)",
journal = "Neurocomputing",
volume = "307",
pages = "72 - 77",
year = "2018",
issn = "0925-2312",
doi = "https://doi.org/10.1016/j.neucom.2018.03.067",
url = "http://www.sciencedirect.com/science/article/pii/S0925231218304843",
author = "Maximilian Christ and Nils Braun and Julius Neuffer and Andreas W. Kempa-Liehr",
keywords = "Feature engineering, Time series, Feature extraction, Feature selection, Machine learning",
}
@article{Christ2016,
  author    = {Maximilian Christ and
               Andreas W. Kempa{-}Liehr and
               Michael Feindt},
  title     = {Distributed and parallel time series feature extraction for industrial
               big data applications},
  journal   = {CoRR},
  volume    = {abs/1610.07717},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.07717},
  archivePrefix = {arXiv},
  eprint    = {1610.07717},
  timestamp = {Mon, 13 Aug 2018 16:46:42 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ChristKF16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Branco2016,
 author = {Branco, Paula and Torgo, Lu\'{\i}s and Ribeiro, Rita P.},
 title = {A Survey of Predictive Modeling on Imbalanced Domains},
 journal = {ACM Comput. Surv.},
 issue_date = {November 2016},
 volume = {49},
 number = {2},
 month = aug,
 year = {2016},
 issn = {0360-0300},
 pages = {31:1--31:50},
 articleno = {31},
 numpages = {50},
 url = {http://doi.acm.org/10.1145/2907070},
 doi = {10.1145/2907070},
 acmid = {2907070},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Imbalanced domains, classification, performance metrics, rare cases, regression},
}
@inproceedings{Chen2016,
 author = {Chen, Tianqi and Guestrin, Carlos},
 title = {XGBoost: A Scalable Tree Boosting System},
 booktitle = {Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '16},
 year = {2016},
 isbn = {978-1-4503-4232-2},
 location = {San Francisco, California, USA},
 pages = {785--794},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2939672.2939785},
 doi = {10.1145/2939672.2939785},
 acmid = {2939785},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {large-scale machine learning},
}
@inproceedings{Yuan2018,
 author = {Yuan, Zhuoning and Zhou, Xun and Yang, Tianbao},
 title = {Hetero-ConvLSTM: A Deep Learning Approach to Traffic Accident Prediction on Heterogeneous Spatio-Temporal Data},
 booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
 series = {KDD '18},
 year = {2018},
 isbn = {978-1-4503-5552-0},
 location = {London, United Kingdom},
 pages = {984--992},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/3219819.3219922},
 doi = {10.1145/3219819.3219922},
 acmid = {3219922},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {convolutional lstm, deep learning, spatial heterogeneity, traffic accident prediction},
}
@incollection{Louppe2013,
title = {Understanding variable importances in forests of randomized trees},
author = {Louppe, Gilles and Wehenkel, Louis and Sutera, Antonio and Geurts, Pierre},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {431--439},
year = {2013},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4928-understanding-variable-importances-in-forests-of-randomized-trees.pdf}
}
@Article{White1994,
author="White, Allan P.
and Liu, Wei Zhong",
title="Technical Note: Bias in Information-Based Measures in Decision Tree Induction",
journal="Machine Learning",
year="1994",
month="Jun",
day="01",
volume="15",
number="3",
pages="321--329",
abstract="A fresh look is taken at the problem of bias in information-based attribute selection measures, used in the induction of decision trees. The approach uses statistical simulation techniques to demonstrate that the usual measures such as information gain, gain ratio, and a new measure recently proposed by Lopez de Mantaras (1991) are all biased in favour of attributes with large numbers of values. It is concluded that approaches which utilise the chi-square distribution are preferable because they compensate automatically for differences between attributes in the number of levels they take.",
issn="1573-0565",
doi="10.1023/A:1022694010754",
url="https://doi.org/10.1023/A:1022694010754"
}
@Article{Strobl2008,
author="Strobl, Carolin
and Boulesteix, Anne-Laure
and Kneib, Thomas
and Augustin, Thomas
and Zeileis, Achim",
title="Conditional variable importance for random forests",
journal="BMC Bioinformatics",
year="2008",
month="Jul",
day="11",
volume="9",
number="1",
pages="307",
abstract="Random forests are becoming increasingly popular in many scientific fields because they can cope with ``small n large p'' problems, complex interactions and even highly correlated predictor variables. Their variable importance measures have recently been suggested as screening tools for, e.g., gene expression studies. However, these variable importance measures show a bias towards correlated predictor variables.",
issn="1471-2105",
doi="10.1186/1471-2105-9-307",
url="https://doi.org/10.1186/1471-2105-9-307"
}
