{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from accidents_montreal import fetch_accidents_montreal,\\\n",
    "                               extract_accidents_montreal_dataframe\n",
    "from road_network import fetch_road_network, extract_road_segments_DF\n",
    "from weather import get_weather\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import atan2, sqrt, row_number, cos, sin, radians,\\\n",
    "                                  col, rank, avg\n",
    "from pyspark.sql import functions, types, Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip fetching montreal accidents dataset: already downloaded\n",
      "Skip extraction of accidents montreal dataframe: already done, reading from file\n"
     ]
    }
   ],
   "source": [
    "def init_spark():\n",
    "    return (SparkSession\n",
    "            .builder\n",
    "            .getOrCreate())\n",
    "\n",
    "\n",
    "# init spark\n",
    "spark = init_spark()\n",
    "\n",
    "# retrieve datasets\n",
    "fetch_accidents_montreal()\n",
    "accidents_df = extract_accidents_montreal_dataframe(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Test pour un seul element\\nrow = test.sample(0.1).first()\\nrow = get_weather(int(row.LOC_LAT), \\n            int(row.LOC_LONG), \\n            int(row.year), \\n            int(row.month), \\n            int(row.day), \\n            int(row.HEURE_ACCDN))\\nprint(row)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def extract_date_val(i):\n",
    "    return udf(lambda val : val.split('/')[i])\n",
    "\n",
    "@udf\n",
    "def extract_hour(val):\n",
    "    return val.split('-')[0].split(':')[0]\n",
    "\n",
    "test = accidents_df.select('DT_ACCDN','LOC_LAT', 'LOC_LONG', 'HEURE_ACCDN')  \\\n",
    "                            .withColumn(\"year\", extract_date_val(0)(accidents_df.DT_ACCDN)) \\\n",
    "                            .withColumn(\"month\", extract_date_val(1)(accidents_df.DT_ACCDN)) \\\n",
    "                            .withColumn(\"day\", extract_date_val(2)(accidents_df.DT_ACCDN)) \\\n",
    "                            .withColumn(\"HEURE_ACCDN\", extract_hour(accidents_df.HEURE_ACCDN)) \\\n",
    "                            .drop('DT_ACCDN') \\\n",
    "                            .replace('Non précisé', '00')\n",
    "\n",
    "def ai(row):\n",
    "    try:\n",
    "        return get_weather(int(row.LOC_LAT), \n",
    "                                 int(row.LOC_LONG), \n",
    "                                 int(row.year), \n",
    "                                 int(row.month), \n",
    "                                 int(row.day), \n",
    "                                 int(row.HEURE_ACCDN))\n",
    "    except:\n",
    "        print('error occured')\n",
    "        return row\n",
    "\n",
    "test2 = spark.createDataFrame(test.limit(1).collect(), test.columns)\n",
    "test2 = test2.rdd \\\n",
    "    .map(lambda row: ai(row)) \n",
    "\n",
    "\"\"\"\n",
    "# Test pour un seul element\n",
    "row = test.sample(0.1).first()\n",
    "row = get_weather(int(row.LOC_LAT), \n",
    "            int(row.LOC_LONG), \n",
    "            int(row.year), \n",
    "            int(row.month), \n",
    "            int(row.day), \n",
    "            int(row.HEURE_ACCDN))\n",
    "print(row)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t=time.time()\n",
    "print(test2.count())\n",
    "t=time.time()-t\n",
    "a=test2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(a.asDict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "it = test2.collect()\n",
    "for i in it:\n",
    "    print(i['Stn Press Flag'])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"test4.write.parquet('data/test.parquet')\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test3 = spark.createDataFrame(test2.collect(), list(a.asDict().keys()))\n",
    "\"\"\"test4.write.parquet('data/test.parquet')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-8-bcfd352dfc07>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-bcfd352dfc07>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    print(get_weather(lat, long, year, month, day, hour))\"\"\"\"\u001b[0m\n\u001b[0m                                                             \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"df=extract_accidents_montreal_dataframe(spark)\n",
    "test=df.take(5)\n",
    "test = list(map(lambda e: (e.LOC_LAT, e.LOC_LONG), test))\n",
    "t= test[0]\n",
    "lat, long = t\n",
    "year, month, day = (2006,5,2)\n",
    "hour = 0\n",
    "print(get_weather(lat, long, year, month, day, hour))\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import os\n",
    "os.path.isfile('data/test.parquet')\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AccidentsPredictionEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
